* Table of Contents :TOC:QUOTE:
#+BEGIN_QUOTE
- [[#links][Links]]
- [[#getting-started][Getting Started]]
  - [[#why-async][Why async]]
  - [[#the-state-of-asynchronous-rust][The State of Asynchronous Rust]]
  - [[#asyncawait-primer][~async/.await~ Primer]]
- [[#asyncawait][async/.await]]
  - [[#async-lifetimes][~async~ Lifetimes]]
  - [[#async-move][~async move~]]
  - [[#awaiting-on-a-multithreaded-executor][~.await~ing on a Multithreaded Executor]]
- [[#the-stream-trait][The ~Stream~ Trait]]
  - [[#iteration-and-concurrency][Iteration and Concurrency]]
- [[#executing-multiple-futures-at-a-time][Executing Multiple Futures at a Time]]
  - [[#join][~join~]]
  - [[#select][~select!~]]
  - [[#spawning][Spawning]]
- [[#workarounds-to-know][Workarounds to know]]
  - [[#-in-async-blocks][~?~ in ~async~ Blocks]]
  - [[#send-approximation][Send Approximation]]
  - [[#async-in-traits][~async~ in Traits]]
- [[#the-async-ecosysten][The Async Ecosysten]]
  - [[#the-futures-crate][The Futures Crate]]
- [[#tokio][Tokio]]
  - [[#tasks][Tasks]]
  - [[#backpressure-and-bounded-channels][Backpressure and bounded channels]]
#+END_QUOTE

* Links

- https://rust-lang.github.io/async-book/

* Getting Started
** Why async
*** Async in Rust vs other languages

Although asynchronous programming is supported in many languages, some details
vary across implementations. Rust's implementation of async differs from most
languages in a few ways:

- *Futures are inert* in Rust and make progress only when polled. Dropping a
  future stops it from making further progress.
- *Async is zero-cost* in Rust, which means that you only pay for what you use.
  Specifically, you can use async without heap allocations and dynamic dispatch,
  which is great for performance! This also lets you use async in constrained
  environments, such as embedded systems.
- *No built-in runtime* is provided by Rust. Instead, runtimes are provided by
  community maintained crates.
- *Both single- and multithreaded* runtimes are available in Rust, which have
  different strengths and weaknesses.

*** Async vs threads in Rust

OS threads are suitable for a small number of tasks, since threads come with CPU
and memory overhead. Spawning and switching between threads is quite expensive
as even idle threads consume system resources. A thread pool library can help
mitigate some of these costs, but not all. However, threads let you reuse
existing synchronous code without significant code changes—no particular
programming model is required. In some operating systems, you can also change
the priority of a thread, which is useful for drivers and other latency
sensitive applications.

Async provides significantly reduced CPU and memory overhead, especially for
workloads with a large amount of IO-bound tasks, such as servers and databases.
All else equal, you can have orders of magnitude more tasks than OS threads,
because an async runtime uses a small amount of (expensive) threads to handle a
large amount of (cheap) tasks. However, async Rust results in larger binary
blobs due to the state machines generated from async functions and since each
executable bundles an async runtime.

On a last note, asynchronous programming is not /better/ than threads, but
different. If you don't need async for performance reasons, threads can often be
the simpler alternative.

*** Example: Concurrent downloading

In this example our goal is to download two web pages concurrently. In a typical
threaded application we need to spawn threads to achieve concurrency:

#+BEGIN_SRC rust :noeval
fn get_two_sites() {
    // Spawn two threads to do work.
    let thread_one = thread::spawn(|| download("https://www.foo.com"));
    let thread_two = thread::spawn(|| download("https://www.bar.com"));

    // Wait for both threads to complete.
    thread_one.join().expect("thread one panicked");
    thread_two.join().expect("thread two panicked");
}
#+END_SRC

However, downloading a web page is a small task; creating a thread for such a
small amount of work is quite wasteful. For a larger application, it can easily
become a bottleneck. In async Rust, we can run these tasks concurrently without
extra threads:

#+BEGIN_SRC rust :noeval
async fn get_two_sites_async() {
    // Create two different "futures" which, when run to completion,
    // will asynchronously download the webpages.
    let future_one = download_async("https://www.foo.com");
    let future_two = download_async("https://www.bar.com");

    // Run both futures to completion at the same time.
    join!(future_one, future_two);
}
#+END_SRC

Here, no extra threads are created. Additionally, all function calls are
statically dispatched, and there are no heap allocations! However, we need to
write the code to be asynchronous in the first place.

Rust doesn't force you to choose between threads and async. You can use both
models within the same application, which can be useful when you have mixed
threaded and async dependencies. In fact, you can even use a different
concurrency model altogether, such as event-driven programming, as long as you
find a library that implements it.

** The State of Asynchronous Rust

In short, async Rust is more difficult to use and can result in a higher
maintenance burden than synchronous Rust, but gives you best-in-class
performance in return. All areas of async Rust are constantly improving, so the
impact of these issues will wear off over time.

*** Language and library support

While asynchronous programming is supported by Rust itself, most async
applications depend on functionality provided by community crates. As such, you
need to rely on a mixture of language features and library support:

- The most fundamental traits, types and functions, such as the [[https://doc.rust-lang.org/std/future/trait.Future.html][Future]] trait are
  provided by the standard library.
- The ~async/await~ syntax is supported directly by the Rust compiler.
- Many utility types, macros and functions are provided by the [[https://docs.rs/futures/][futures]] crate.
  They can be used in any async Rust application.
- Execution of async code, IO and task spawning are provided by "async
  runtimes", such as Tokio and async-std. Most async applications, and some
  async crates, depend on a specific runtime.

*** Compatibility considerations

Asynchronous and synchronous code cannot always be combined freely. For
instance, you can't directly call an async function from a sync function. Sync
and async code also tend to promote different design patterns, which can make it
difficult to compose code intended for the different environments.

Even async code cannot always be combined freely. Some crates depend on a
specific async runtime to function. If so, it is usually specified in the
crate's dependency list.

These compatibility issues can limit your options, so make sure to research
which async runtime and what crates you may need early. Once you have settled in
with a runtime, you won't have to worry much about compatibility.

** ~async/.await~ Primer

~async~ / ~.await~ is Rust's built-in tool for writing asynchronous functions
that look like synchronous code. ~async~ transforms a block of code into a state
machine that implements a trait called ~Future~. Whereas calling a blocking
function in a synchronous method would block the whole thread, blocked ~Future~s
will yield control of the thread, allowing other ~Future~s to run.

Let's add some dependencies to the ~Cargo.toml~ file:

#+BEGIN_SRC toml
[dependencies]
futures = "0.3"
#+END_SRC

To create an asynchronous function, you can use the async fn syntax:

#+BEGIN_SRC rust :noeval
async fn do_something() { /* ... */ }
#+END_SRC

The value returned by ~async fn~ is a ~Future~. For anything to happen, the
~Future~ needs to be run on an executor.

#+BEGIN_SRC rust :results output
// `block_on` blocks the current thread until the provided future has run to
// completion. Other executors provide more complex behavior, like scheduling
// multiple futures onto the same thread.
use futures::executor::block_on;

async fn hello_world() {
    println!("hello, world!");
}

fn main() {
    let future = hello_world(); // Nothing is printed
    block_on(future); // `future` is run and "hello, world!" is printed
}
#+END_SRC

Inside an ~async fn~, you can use ~.await~ to wait for the completion of another
type that implements the ~Future~ trait, such as the output of another
~async fn~. Unlike ~block_on~, ~.await~ doesn't block the current thread, but
instead asynchronously waits for the future to complete, allowing other tasks to
run if the future is currently unable to make progress.

For example, imagine that we have three ~async fn~: ~learn_song~, ~sing_song~,
and ~dance~:

#+BEGIN_SRC rust :noeval
async fn learn_song() -> Song { /* ... */ }
async fn sing_song(song: Song) { /* ... */ }
async fn dance() { /* ... */ }
#+END_SRC

One way to do learn, sing, and dance would be to block on each of these
individually:

#+BEGIN_SRC rust :noeval
fn main() {
    let song = block_on(learn_song());
    block_on(sing_song(song));
    block_on(dance());
}
#+END_SRC

However, we're not giving the best performance possible this way—we're only ever
doing one thing at once! Clearly we have to learn the song before we can sing
it, but it's possible to dance at the same time as learning and singing the
song. To do this, we can create two separate async fn which can be run
concurrently:

#+BEGIN_SRC rust :noeval
async fn learn_and_sing() {
    // Wait until the song has been learned before singing it.
    // We use `.await` here rather than `block_on` to prevent blocking the
    // thread, which makes it possible to `dance` at the same time.
    let song = learn_song().await;
    sing_song(song).await;
}

async fn async_main() {
    let f1 = learn_and_sing();
    let f2 = dance();

    // `join!` is like `.await` but can wait for multiple futures concurrently.
    // If we're temporarily blocked in the `learn_and_sing` future, the `dance`
    // future will take over the current thread. If `dance` becomes blocked,
    // `learn_and_sing` can take back over. If both futures are blocked, then
    // `async_main` is blocked and will yield to the executor.
    futures::join!(f1, f2);
}

fn main() {
    block_on(async_main());
}
#+END_SRC

In this example, learning the song must happen before singing the song, but both
learning and singing can happen at the same time as dancing. If we used
~block_on(learn_song())~ rather than ~learn_song().await~ in ~learn_and_sing~,
the thread wouldn't be able to do anything else while ~learn_song~ was running.
This would make it impossible to ~dance~ at the same time. By ~.await~-ing the
~learn_song~ future, we allow other tasks to take over the current thread if
~learn_song~ is blocked. This makes it possible to run multiple futures to
completion concurrently on the same thread.

* async/.await

~async~ / ~.await~ are special pieces of Rust syntax that make it possible to
yield control of the current thread rather than blocking, allowing other code to
make progress while waiting on an operation to complete.

There are two main ways to use ~async~: ~async fn~ and ~async~ blocks. Each
returns a value that implements the ~Future~ trait:

#+BEGIN_SRC rust :noeval
// `foo()` returns a type that implements `Future<Output = u8>`.
// `foo().await` will result in a value of type `u8`.
async fn foo() -> u8 { 5 }

fn bar() -> impl Future<Output = u8> {
    // This `async` block results in a type that implements
    // `Future<Output = u8>`.
    async {
        let x: u8 = foo().await;
        x + 5
    }
}
#+END_SRC

As we saw in the first chapter, ~async~ bodies and other futures are lazy: they
do nothing until they are run. The most common way to run a ~Future~ is to
~.await~ it. When ~.await~ is called on a ~Future~, it will attempt to run it to
completion. If the ~Future~ is blocked, it will yield control of the current
thread. When more progress can be made, the ~Future~ will be picked up by the
executor and will resume running, allowing the ~.await~ to resolve.

** ~async~ Lifetimes

Unlike traditional functions, ~async fn~s which take references or other
non-~'static~ arguments return a ~Future~ which is bounded by the lifetime of
the arguments:

#+BEGIN_SRC rust :noeval
// This function:
async fn foo(x: &u8) -> u8 { *x }

// Is equivalent to this function:
fn foo_expanded<'a>(x: &'a u8) -> impl Future<Output = u8> + 'a {
    async move { *x }
}
#+END_SRC

This means that the future returned from an ~async fn~ must be ~.awaited~ while
its non-~'static~ arguments are still valid. In the common case of ~.await~ing
the future immediately after calling the function (as in ~foo(&x).await~) this
is not an issue. However, if storing the future or sending it over to another
task or thread, this may be an issue.

One common workaround for turning an ~async fn~ with references-as-arguments
into a ~'static~ future is to bundle the arguments with the call to the
~async fn~ inside an ~async~ block:

#+BEGIN_SRC rust :noeval
fn bad() -> impl Future<Output = u8> {
    let x = 5;
    borrow_x(&x) // ERROR: `x` does not live long enough
}

fn good() -> impl Future<Output = u8> {
    async {
        let x = 5;
        borrow_x(&x).await
    }
}
#+END_SRC

By moving the argument into the ~async~ block, we extend its lifetime to match
that of the ~Future~ returned from the call to ~good~.

** ~async move~

~async~ blocks and closures allow the ~move~ keyword, much like normal closures.
An ~async move~ block will take ownership of the variables it references,
allowing it to outlive the current scope, but giving up the ability to share
those variables with other code:

#+BEGIN_SRC rust :noeval
/// `async` block:
///
/// Multiple different `async` blocks can access the same local variable
/// so long as they're executed within the variable's scope
async fn blocks() {
    let my_string = "foo".to_string();

    let future_one = async {
        // ...
        println!("{my_string}");
    };

    let future_two = async {
        // ...
        println!("{my_string}");
    };

    // Run both futures to completion, printing "foo" twice:
    let ((), ()) = futures::join!(future_one, future_two);
}

/// `async move` block:
///
/// Only one `async move` block can access the same captured variable, since
/// captures are moved into the `Future` generated by the `async move` block.
/// However, this allows the `Future` to outlive the original scope of the
/// variable:
fn move_block() -> impl Future<Output = ()> {
    let my_string = "foo".to_string();
    async move {
        // ...
        println!("{my_string}");
    }
}
#+END_SRC

** ~.await~ing on a Multithreaded Executor

Note that, when using a multithreaded ~Future~ executor, a ~Future~ may move
between threads, so any variables used in ~async~ bodies must be able to travel
between threads, as any ~.await~ can potentially result in a switch to a new
thread.

This means that it is not safe to use ~Rc~, ~&RefCell~ or any other types that
don't implement the ~Send~ trait, including references to types that don't
implement the ~Sync~ trait.

(Caveat: it is possible to use these types as long as they aren't in scope
during a call to ~.await~.)

Similarly, it isn't a good idea to hold a traditional non-futures-aware lock
across an ~.await~, as it can cause the threadpool to lock up: one task could
take out a lock, ~.await~ and yield to the executor, allowing another task to
attempt to take the lock and cause a deadlock. To avoid this, use the ~Mutex~
in ~futures::lock~ rather than the one from ~std::sync~.

* The ~Stream~ Trait

The ~Stream~ trait is similar to ~Future~ but can yield multiple values before
completing, similar to the ~Iterator~ trait from the standard library:

#+BEGIN_SRC rust :noeval
trait Stream {
    /// The type of the value yielded by the stream.
    type Item;

    /// Attempt to resolve the next item in the stream.
    /// Returns `Poll::Pending` if not ready, `Poll::Ready(Some(x))` if a value
    /// is ready, and `Poll::Ready(None)` if the stream has completed.
    fn poll_next(self: Pin<&mut Self>, cx: &mut Context<'_>)
        -> Poll<Option<Self::Item>>;
}
#+END_SRC

One common example of a ~Stream~ is the ~Receiver~ for the channel type from the
~futures~ crate. It will yield ~Some(val)~ every time a value is sent from the
~Sender~ end, and will yield ~None~ once the ~Sender~ has been dropped and all
pending messages have been received:

#+BEGIN_SRC rust :noeval
async fn send_recv() {
    const BUFFER_SIZE: usize = 10;
    let (mut tx, mut rx) = mpsc::channel::<i32>(BUFFER_SIZE);

    tx.send(1).await.unwrap();
    tx.send(2).await.unwrap();
    drop(tx);

    // `StreamExt::next` is similar to `Iterator::next`, but returns a
    // type that implements `Future<Output = Option<T>>`.
    assert_eq!(Some(1), rx.next().await);
    assert_eq!(Some(2), rx.next().await);
    assert_eq!(None, rx.next().await);
}
#+END_SRC

** Iteration and Concurrency

Similar to synchronous ~Iterator~s, there are many different ways to iterate
over and process the values in a ~Stream~. There are combinator-style methods
such as ~map~, ~filter~, and ~fold~, and their early-exit-on-error cousins
~try_map~, ~try_filter~, and ~try_fold~.

Unfortunately, ~for~ loops are not usable with ~Stream~s, but for
imperative-style code, ~while let~ and the ~next~ / ~try_next~ functions can be
used:

#+BEGIN_SRC rust :noeval
async fn sum_with_next(mut stream: Pin<&mut dyn Stream<Item = i32>>) -> i32 {
    use futures::stream::StreamExt; // for `next`
    let mut sum = 0;
    while let Some(item) = stream.next().await {
        sum += item;
    }
    sum
}

async fn sum_with_try_next(
    mut stream: Pin<&mut dyn Stream<Item = Result<i32, io::Error>>>,
) -> Result<i32, io::Error> {
    use futures::stream::TryStreamExt; // for `try_next`
    let mut sum = 0;
    while let Some(item) = stream.try_next().await? {
        sum += item;
    }
    Ok(sum)
}
#+END_SRC

However, if we're just processing one element at a time, we're potentially
leaving behind opportunity for concurrency, which is, after all, why we're
writing async code in the first place. To process multiple items from a stream
concurrently, use the ~for_each_concurrent~ and ~try_for_each_concurrent~
methods:

#+BEGIN_SRC rust
async fn jump_around(
    mut stream: Pin<&mut dyn Stream<Item = Result<u8, io::Error>>>,
) -> Result<(), io::Error> {
    use futures::stream::TryStreamExt; // for `try_for_each_concurrent`
    const MAX_CONCURRENT_JUMPERS: usize = 100;

    stream.try_for_each_concurrent(MAX_CONCURRENT_JUMPERS, |num| async move {
        jump_n_times(num).await?;
        report_n_jumps(num).await?;
        Ok(())
    }).await?;

    Ok(())
}
#+END_SRC

* Executing Multiple Futures at a Time

Up until now, we've mostly executed futures by using ~.await~, which blocks the
current task until a particular ~Future~ completes. However, real asynchronous
applications often need to execute several different operations concurrently.

- ~join!~: waits for futures to all complete
- ~select!~: waits for one of several futures to complete
- Spawning: creates a top-level task which ambiently runs a future to completion
- ~FuturesUnordered~: a group of futures which yields the result of each
  subfuture (not described yet)

** ~join~

The ~futures::join~ macro makes it possible to wait for multiple different
futures to complete while executing them all concurrently.

When performing multiple asynchronous operations, it's tempting to simply
~.await~ them in a series:

#+BEGIN_SRC rust :noeval
async fn get_book_and_music() -> (Book, Music) {
    let book = get_book().await;
    let music = get_music().await;
    (book, music)
}
#+END_SRC

However, this will be slower than necessary, since it won't start trying to
~get_music~ until after ~get_book~ has completed. Rust futures won't do any work
until they're actively ~.await~ed. This means that it doesn't matter if we
create both ~Future~s first and then ~.await~ them. To correctly run the two
futures concurrently, use ~futures::join!~:

#+BEGIN_SRC rust :noeval
use futures::join;

async fn get_book_and_music() -> (Book, Music) {
    let book_fut = get_book();
    let music_fut = get_music();
    join!(book_fut, music_fut)
}
#+END_SRC

The value returned by ~join!~ is a tuple containing the output of each ~Future~
passed in.

For futures which return ~Result~, consider using ~try_join!~ rather than
~join!~. Since ~join!~ only completes once all subfutures have completed, it'll
continue processing other futures even after one of its subfutures has returned
an ~Err~.

Unlike ~join!~, ~try_join!~ will complete immediately if one of the subfutures
returns an error.

#+BEGIN_SRC rust :noeval
use futures::try_join;

async fn get_book() -> Result<Book, String> { /* ... */ Ok(Book) }
async fn get_music() -> Result<Music, String> { /* ... */ Ok(Music) }

async fn get_book_and_music() -> Result<(Book, Music), String> {
    let book_fut = get_book();
    let music_fut = get_music();
    try_join!(book_fut, music_fut)
}
#+END_SRC

Note that the futures passed to ~try_join!~ must all have the same error type.
Consider using the ~.map_err(|e| ...)~ and ~.err_into()~ functions from
~futures::future::TryFutureExt~ to consolidate the error types:

#+BEGIN_SRC rust :noeval
use futures::{
    future::TryFutureExt,
    try_join,
};

async fn get_book() -> Result<Book, ()> { /* ... */ Ok(Book) }
async fn get_music() -> Result<Music, String> { /* ... */ Ok(Music) }

async fn get_book_and_music() -> Result<(Book, Music), String> {
    let book_fut = get_book().map_err(|()| "Unable to get book".to_string());
    let music_fut = get_music();
    try_join!(book_fut, music_fut)
}
#+END_SRC

** ~select!~

The ~futures::select~ macro runs multiple futures simultaneously, allowing the
user to respond as soon as any future completes.

#+BEGIN_SRC rust :noeval
use futures::{
    future::FutureExt, // for `.fuse()`
    pin_mut,
    select,
};

async fn task_one() { /* ... */ }
async fn task_two() { /* ... */ }

async fn race_tasks() {
    let t1 = task_one().fuse();
    let t2 = task_two().fuse();

    pin_mut!(t1, t2);

    select! {
        () = t1 => println!("task one completed first"),
        () = t2 => println!("task two completed first"),
    }
}
#+END_SRC

The function above will run both ~t1~ and ~t2~ concurrently. When either ~t1~ or
~t2~ finishes, the corresponding handler will call ~println!~, and the function
will end without completing the remaining task.

The basic syntax for ~select~ is ~<pattern> = <expression> => <code>,~, repeated
for as many futures as you would like to ~select~ over.

*** ~default~ and ~complete~

~select~ also supports ~default~ and ~complete~ branches.

A ~default~ branch will run if none of the futures being ~select~ed over are yet
complete. A ~select~ with a ~default~ branch will therefore always return
immediately, since ~default~ will be run if none of the other futures are ready.

~complete~ branches can be used to handle the case where all futures being
~select~ed over have completed and will no longer make progress. This is often
handy when looping over a ~select!~.

#+BEGIN_SRC rust :noeval
use futures::{future, select};

async fn count() {
    let mut a_fut = future::ready(4);
    let mut b_fut = future::ready(6);
    let mut total = 0;

    loop {
        select! {
            a = a_fut => total += a,
            b = b_fut => total += b,
            complete => break,
            default => unreachable!(), // never runs (futures are ready, then complete)
        };
    }
    assert_eq!(total, 10);
}
#+END_SRC

*** Interaction with ~Unpin~ and ~FusedFuture~

One thing you may have noticed in the first example above is that we had to call
~.fuse()~ on the futures returned by the two ~async fn~, as well as [[https://rust-lang.github.io/async-book/04_pinning/01_chapter.html][pinning]]
them with ~pin_mut~. Both of these calls are necessary because the futures used
in ~select~ must implement both the ~Unpin~ trait and the ~FusedFuture~ trait.

~Unpin~ is necessary because the futures used by ~select~ are not taken by
value, but by mutable reference. By not taking ownership of the future,
uncompleted futures can be used again after the call to ~select~.

Similarly, the ~FusedFuture~ trait is required because ~select~ must not poll a
future after it has completed. ~FusedFuture~ is implemented by futures which
track whether or not they have completed. This makes it possible to use ~select~
in a loop, only polling the futures which still have yet to complete.

Note that streams have a corresponding ~FusedStream~ trait. Streams which
implement this trait or have been wrapped using ~.fuse()~ will yield
~FusedFuture~ futures from their ~.next()~ / ~.try_next()~ combinators.

#+BEGIN_SRC rust :noeval
use futures::{
    stream::{Stream, StreamExt, FusedStream},
    select,
};

async fn add_two_streams(
    mut s1: impl Stream<Item = u8> + FusedStream + Unpin,
    mut s2: impl Stream<Item = u8> + FusedStream + Unpin,
) -> u8 {
    let mut total = 0;

    loop {
        let item = select! {
            x = s1.next() => x,
            x = s2.next() => x,
            complete => break,
        };
        if let Some(next_num) = item {
            total += next_num;
        }
    }

    total
}
#+END_SRC

** Spawning

Spawning allows you to run a new asynchronous task in the background. This
allows us to continue executing other code while it runs.

Say we have a web server that wants to accept connections without blocking the
main thread. To achieve this, we can use the ~async_std::task::spawn~ function
to create and run a new task that handles the connections. This function takes a
future and returns a ~JoinHandle~, which can be used to wait for the result of
the task once it's completed.

#+BEGIN_SRC rust :noeval
use async_std::{task, net::TcpListener, net::TcpStream};
use futures::AsyncWriteExt;

async fn process_request(stream: &mut TcpStream) -> Result<(), std::io::Error>{
    stream.write_all(b"HTTP/1.1 200 OK\r\n\r\n").await?;
    stream.write_all(b"Hello World").await?;
    Ok(())
}

async fn main() {
    let listener = TcpListener::bind("127.0.0.1:8080").await.unwrap();
    loop {
        // Accept a new connection
        let (mut stream, _) = listener.accept().await.unwrap();
        // Now process this request without blocking the main loop
        task::spawn(async move {process_request(&mut stream).await});
    }
}
#+END_SRC

The ~JoinHandle~ returned by ~spawn~ implements the ~Future~ trait, so we can
~.await~ it to get the result of the task. This will block the current task
until the spawned task completes. If the task is not awaited, your program will
continue executing without waiting for the task, cancelling it if the function
is completed before the task is finished.

#+BEGIN_SRC rust :noeval
use futures::future::join_all;
async fn task_spawner(){
    let tasks = vec![
        task::spawn(my_task(Duration::from_secs(1))),
        task::spawn(my_task(Duration::from_secs(2))),
        task::spawn(my_task(Duration::from_secs(3))),
    ];
    // If we do not await these tasks and the function finishes, they will be dropped
    join_all(tasks).await;
}
#+END_SRC

To communicate between the main task and the spawned task, we can use channels
provided by the async runtime used.

* Workarounds to know
** ~?~ in ~async~ Blocks

Just as in ~async fn~, it's common to use ~?~ inside ~async~ blocks. However,
the return type of ~async~ blocks isn't explicitly stated. This can cause the
compiler to fail to infer the error type of the ~async~ block.

For example, this code:

#+BEGIN_SRC rust :noeval
let fut = async {
    foo().await?;
    bar().await?;
    Ok(())
};
#+END_SRC

will trigger this error:

#+BEGIN_SRC
error[E0282]: type annotations needed
 --> src/main.rs:5:9
  |
4 |     let fut = async {
  |         --- consider giving `fut` a type
5 |         foo().await?;
  |         ^^^^^^^^^^^^ cannot infer type
#+END_SRC

Unfortunately, there's currently no way to "give ~fut~ a type", nor a way to
explicitly specify the return type of an ~async~ block. To work around this, use
the "turbofish" operator to supply the success and error types for the ~async~
block:

#+BEGIN_SRC rust :noeval
let fut = async {
    foo().await?;
    bar().await?;
    Ok::<(), MyError>(()) // <- note the explicit type annotation here
};
#+END_SRC

** Send Approximation

Some ~async fn~ state machines are safe to be sent across threads, while others
are not. Whether or not an ~async fn~ ~Future~ is ~Send~ is determined by
whether a non-~Send~ type is held across an ~.await~ point. The compiler does
its best to approximate when values may be held across an ~.await~ point, but
this analysis is too conservative in a number of places today.

For example, consider a simple non-~Send~ type, perhaps a type which contains
an ~Rc~:

#+BEGIN_SRC rust :noeval
use std::rc::Rc;

#[derive(Default)]
struct NotSend(Rc<()>);
#+END_SRC

Variables of type ~NotSend~ can briefly appear as temporaries in ~async fn~ even
when the resulting ~Future~ type returned by the ~async fn~ must be ~Send~:

#+BEGIN_SRC rust :results output
async fn bar() {}
async fn foo() {
    NotSend::default();
    bar().await;
}

fn require_send(_: impl Send) {}

fn main() {
    require_send(foo());
}
#+END_SRC

However, if we change ~foo~ to store ~NotSend~ in a variable, this example no
longer compiles:

#+BEGIN_SRC rust :noeval
async fn foo() {
    let x = NotSend::default();
    bar().await;
}
#+END_SRC

If we store ~x~ into a variable, it won't be dropped until after the ~.await~,
at which point the ~async fn~ may be running on a different thread. Since ~Rc~
is not ~Send~, allowing it to travel across threads would be unsound. One simple
solution to this would be to ~drop~ the ~Rc~ before the ~.await~, but
unfortunately that does not work today.

In order to successfully work around this issue, you may have to introduce a
block scope encapsulating any non-~Send~ variables. This makes it easier for the
compiler to tell that these variables do not live across an ~.await~ point.

#+BEGIN_SRC rust :noeval
async fn foo() {
    {
        let x = NotSend::default();
    }
    bar().await;
}
#+END_SRC

** ~async~ in Traits

Currently, ~async fn~ cannot be used in traits on the stable release of Rust. In
the meantime, there is a work around for the stable tool chain using the
[[https://github.com/dtolnay/async-trait][async-trait crate from crates.io]].

* The Async Ecosysten
** The Futures Crate

The [[https://docs.rs/futures/][~futures~ crate]] contains traits and functions useful for writing async code.
This includes the ~Stream~, ~Sink~, ~AsyncRead~, and ~AsyncWrite~ traits, and
utilities such as combinators. These utilities and traits may eventually become
part of the standard library.

~futures~ has its own executor, but not its own reactor, so it does not support
execution of async I/O or timer futures. For this reason, it's not considered a
full runtime. A common choice is to use utilities from futures with an executor
from another crate.

* Tokio

Examples to play around with:

** Tasks

A Tokio task is an asynchronous green thread. They are created by passing an
~async~ block to ~tokio::spawn~. The ~tokio::spawn~ function returns a
~JoinHandle~, which the caller may use to interact with the spawned task. The
~async~ block may have a return value. The caller may obtain the return value
using ~.await~ on the ~JoinHandle~.

#+BEGIN_SRC rust :noeval
#[tokio::main]
async fn main() {
    let handle = tokio::spawn(async {
        // Do some async work
        "return value"
    });

    // Do some other work

    let out = handle.await.unwrap();
    println!("GOT {}", out);
}
#+END_SRC

Awaiting on ~JoinHandle~ returns a ~Result~. When a task encounters an error
during execution, the ~JoinHandle~ will return an ~Err~. This happens when the
task either panics, or if the task is forcefully cancelled by the runtime
shutting down.

Tasks are the unit of execution managed by the scheduler. Spawning the task
submits it to the Tokio scheduler, which then ensures that the task executes
when it has work to do. The spawned task may be executed on the same thread as
where it was spawned, or it may execute on a different runtime thread. The task
can also be moved between threads after being spawned.

Tasks in Tokio are very lightweight.

*** ~'static~ bound

When you spawn a task on the Tokio runtime, its type's lifetime must be
~'static~. This means that the spawned task must not contain any references to
data owned outside the task.

It is a common misconception that ~'static~ always means "lives forever", but
this is not the case. Just because a value is ~'static~ does not mean that you
have a memory leak. You can read more in [[https://github.com/pretzelhammer/rust-blog/blob/master/posts/common-rust-lifetime-misconceptions.md#2-if-t-static-then-t-must-be-valid-for-the-entire-program][Common Rust Lifetime Misconceptions]].

When we say that a value is ~'static~, all that means is that it would not be
incorrect to keep that value around forever. This is important because the
compiler is unable to reason about how long a newly spawned task stays around.
We have to make sure that the task is allowed to live forever, so that Tokio can
make the task run as long as it needs to.

*** Example to test

#+BEGIN_SRC rust :noeval
use tokio::task;

#[tokio::main]
async fn main() {
    let v = vec![1, 2, 3];

    task::spawn(async move {
        println!("Here's a vec: {:?}", v);
    });
}
#+END_SRC

*** ~Send~ bound

Tasks spawned by ~tokio::spawn~ must implement ~Send~. This allows the Tokio
runtime to move the tasks between threads while they are suspended at an
~.await~.

Tasks are ~Send~ when all data that is held across ~.await~ calls is ~Send~.
This is a bit subtle. When ~.await~ is called, the task yields back to the
scheduler. The next time the task is executed, it resumes from the point it last
yielded. To make this work, all state that is used after ~.await~ must be saved
by the task. If this state is ~Send~, i.e. can be moved across threads, then the
task itself can be moved across threads. Conversely, if the state is not ~Send~,
then neither is the task.

For example, this works:

#+BEGIN_SRC rust :noeval
use tokio::task::yield_now;
use std::rc::Rc;

#[tokio::main]
async fn main() {
    tokio::spawn(async {
        // The scope forces `rc` to drop before `.await`.
        {
            let rc = Rc::new("hello");
            println!("{}", rc);
        }

        // `rc` is no longer used. It is **not** persisted when
        // the task yields to the scheduler
        yield_now().await;
    });
}
#+END_SRC

This does not:

#+BEGIN_SRC rust :noeval
use tokio::task::yield_now;
use std::rc::Rc;

#[tokio::main]
async fn main() {
    tokio::spawn(async {
        let rc = Rc::new("hello");

        // `rc` is used after `.await`. It must be persisted to
        // the task's state.
        yield_now().await;

        println!("{}", rc);
    });
}
#+END_SRC

** Backpressure and bounded channels

Whenever concurrency or queuing is introduced, it is important to ensure that
the queing is bounded and the system will gracefully handle the load. Unbounded
queues will eventually fill up all available memory and cause the system to fail
in unpredictable ways.

Tokio takes care to avoid implicit queuing. A big part of this is the fact that
async operations are lazy. Consider the following:

#+BEGIN_SRC rust :noeval
loop {
    async_op();
}
#+END_SRC

If the asynchronous operation runs eagerly, the loop will repeatedly queue a new
~async_op~ to run without ensuring the previous operation completed. This
results in implicit unbounded queuing. Callback based systems and *eager* future
based systems are particularly susceptible to this.

However, with Tokio and asynchronous Rust, the above snippet will *not* result
in ~async_op~ running at all. This is because ~.await~ is never called. If the
snippet is updated to use ~.await~, then the loop waits for the operation to
complete before starting over.

#+BEGIN_SRC rust :noeval
loop {
    // Will not repeat until `async_op` completes
    async_op().await;
}
#+END_SRC

Concurrency and queuing must be explicitly introduced. Ways to do this include:

- ~tokio::spawn~
- ~select!~
- ~join!~
- ~mpsc::channel~ (how?)

When doing so, take care to ensure the total amount of concurrency is bounded.
For example, when writing a TCP accept loop, ensure that the total number of
open sockets is bounded. When using ~mpsc::channel~, pick a manageable channel
capacity. Specific bound values will be application specific.

Taking care and picking good bounds is a big part of writing reliable Tokio
applications.
