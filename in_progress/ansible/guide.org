* Table of Contents :TOC:QUOTE:
#+BEGIN_QUOTE
- [[#links][Links]]
- [[#getting-started][Getting started]]
  - [[#install][Install]]
  - [[#configure-ansible][Configure ansible]]
  - [[#create-docker-target-environment][Create docker target environment]]
- [[#building-an-inventory][Building an inventory]]
  - [[#intro][Intro]]
  - [[#how-to-build-your-inventory][How to build your inventory]]
- [[#playbooks][Playbooks]]
- [[#old][Old]]
  - [[#ad-hoc-commands][Ad-hoc commands]]
  - [[#module-documentation][Module documentation]]
  - [[#playbooks-1][Playbooks]]
  - [[#ansible-vault][Ansible vault]]
  - [[#playbook-organization][Playbook organization]]
  - [[#roles][Roles]]
  - [[#config][Config]]
#+END_QUOTE

* Links

- https://www.jeffgeerling.com/blog/2020/ansible-101-jeff-geerling-youtube-streaming-series (ansible 2.9)?
- https://docs.ansible.com/ansible/latest/getting_started/basic_concepts.html (glossary)

* Getting started
** Install

#+BEGIN_SRC bash :noeval
# For fedora
sudo dnf install ansible

# For osx
brew install ansible

# Verify install
ansible --version
#+END_SRC

*** Install on the managed node

Ansible does not have to be installed but you need python and a user account
that can connect through SSH.

** Configure ansible

You don't have to follow these steps as there is already a [[file:ansible.cfg][ansible.cfg]] created.

You can generate an Ansible configuration file, ~ansible.cfg~, that lists all
default settings as follows:

#+BEGIN_SRC bash :noeval
ansible-config init --disabled > ansible.cfg
#+END_SRC

Include available plugins to create a more complete Ansible configuration as
follows:

#+BEGIN_SRC bash :noeval
ansible-config init --disabled -t all > ansible.cfg
#+END_SRC

The configuration files are searched for in the following order:

- ~ANSIBLE_CONFIG~ (environment variable if set)
- ~ansible.cfg~ (in the current directory)
- ~~/.ansible.cfg~ (in the home directory)
- ~/etc/ansible/ansible.cfg~

Not all configuration options are present in the command line, just the ones
deemed most useful or common. Settings in the command line will override those
passed through the configuration file and the environment.

For more information see the [[https://docs.ansible.com/ansible/latest/reference_appendices/config.html#ansible-configuration-settings][Ansible Configuration Settings documentation]].

*** A note about comments in ~ansible.cfg~

The configuration file is one variant of an ~.ini~ format. Both the hash sign
(~#~) and semicolon (~;~) are allowed as comment markers when the comment starts
the line. However, if the comment is inline with regular values, only the
semicolon is allowed to introduce the comment.

** Create docker target environment
*** Ubuntu environment

#+BEGIN_SRC bash :noeval
cd dockerenv/ubuntu
ssh-keygen -b 4096 -t rsa -f ./id_rsa -N ""
docker build -t ubuntuansibletarget:latest .

# Start env. I use --init because it seems to respect ctrl-c when I want to exit
docker run --rm -p 1022:22 --init ubuntuansibletarget:latest

# Test connection
ssh -o "IdentitiesOnly=yes" -o "StrictHostKeyChecking=no" -o "UserKnownHostsFile=/dev/null" -i id_rsa -p 1022 ansibleuser@localhost
#+END_SRC

*** Fedora environment

#+BEGIN_SRC bash :noeval
cd dockerenv/fedora
ssh-keygen -b 4096 -t rsa -f ./id_rsa -N ""
docker build -t fedoraansibletarget:latest .

# Start env. I use --init because it seems to respect ctrl-c when I want to exit
docker run --rm -p 2022:22 --init fedoraansibletarget:latest

# Test connection
ssh -o "IdentitiesOnly=yes" -o "StrictHostKeyChecking=no" -o "UserKnownHostsFile=/dev/null" -i id_rsa -p 2022 ansibleuser@localhost
#+END_SRC

*** Create multiple containers with the ~docker compose~ command

After the containers above has been built you can create multiple of them by
running:

#+BEGIN_SRC bash :noeval
cd dockerenv
docker compose up ; docker compose down
#+END_SRC

This will run the containers in the foreground and will remove the containers
automatically when stopped.

Try connecting to each machine:

#+BEGIN_SRC bash :noeval
ssh -o "IdentitiesOnly=yes" -o "StrictHostKeyChecking=no" -o "UserKnownHostsFile=/dev/null" -i ubuntu/id_rsa -p 1022 ansibleuser@localhost whoami
ssh -o "IdentitiesOnly=yes" -o "StrictHostKeyChecking=no" -o "UserKnownHostsFile=/dev/null" -i ubuntu/id_rsa -p 1122 ansibleuser@localhost whoami
ssh -o "IdentitiesOnly=yes" -o "StrictHostKeyChecking=no" -o "UserKnownHostsFile=/dev/null" -i fedora/id_rsa -p 2022 ansibleuser@localhost whoami
ssh -o "IdentitiesOnly=yes" -o "StrictHostKeyChecking=no" -o "UserKnownHostsFile=/dev/null" -i fedora/id_rsa -p 2122 ansibleuser@localhost whoami
#+END_SRC

* Building an inventory
** Intro

The ~inventory.yaml~ basic format looks something like this:

#+BEGIN_SRC yaml
myhosts: # Group name
  hosts:
    my_host_01:
      ansible_host: 192.0.2.50 # Ip for the my_host_01 alias
    my_host_02:
      ansible_host: 192.0.2.51
    my_host_03:
      ansible_host: 192.0.2.52
#+END_SRC

*** Variables

Variables set values for managed nodes, such as the IP address, FQDN, operating
system, and SSH user, so you do not need to pass them when running Ansible
commands.

Variables can apply to specific hosts.

#+BEGIN_SRC yaml
webservers:
  hosts:
    webserver01:
      ansible_host: 192.0.2.140
      http_port: 80
    webserver02:
      ansible_host: 192.0.2.150
      http_port: 443
#+END_SRC

Variables can also apply to all hosts in a group:

#+BEGIN_SRC yaml
webservers:
  hosts:
    webserver01:
      ansible_host: 192.0.2.140
      http_port: 80
    webserver02:
      ansible_host: 192.0.2.150
      http_port: 443
  vars:
    ansible_user: my_server_user
#+END_SRC

*** Metagroups

Create a metagroup that organizes multiple groups in your inventory with the following syntax:

#+BEGIN_SRC yaml
metagroupname:
  children:
#+END_SRC

The following inventory illustrates a basic structure for a data center. This
example inventory contains a ~network~ metagroup that includes all network
devices and a ~datacenter~ metagroup that includes the ~network~ group and all
webservers.

#+BEGIN_SRC yaml
leafs:
  hosts:
    leaf01:
      ansible_host: 192.0.2.100
    leaf02:
      ansible_host: 192.0.2.110

spines:
  hosts:
    spine01:
      ansible_host: 192.0.2.120
    spine02:
      ansible_host: 192.0.2.130

network:
  children:
    leafs:
    spines:

webservers:
  hosts:
    webserver01:
      ansible_host: 192.0.2.140
    webserver02:
      ansible_host: 192.0.2.150

datacenter:
  children:
    network:
    webservers:
#+END_SRC

*** Example

Start checking the inventory file that has been configured for the container
defined earlier:

#+BEGIN_SRC bash :noeval
ansible-inventory -i inventory/inventory.yaml --list
# Because we have a ansible.cfg file which points to the inventory file we can
# just run
ansible-inventory --list
#+END_SRC

In our inventory I ahve defined some variables for each host as we don't want to
use the defaults.

Try pinging each of them:

#+BEGIN_SRC bash :noeval
ansible all -m ping -i inventory/inventory.yaml
# Because we have a ansible.cfg file which points to the inventory file we can
# just run
ansible all -m ping
#+END_SRC

The inventory can be in both ~.yaml~ and ~.ini~ format. I prefer ~.yaml~ and
will only use ~.yaml~ in my examples.

** How to build your inventory

Ansible automates tasks on managed nodes or “hosts” in your infrastructure,
using a list or group of lists known as inventory. You can pass host names at
the command line, but most Ansible users create inventory files. Your inventory
defines the managed nodes you automate, with groups so you can run automation
tasks on multiple hosts at the same time. Once your inventory is defined, you
use patterns to select the hosts or groups you want Ansible to run against.

The default location for this file is ~/etc/ansible/hosts~. You can specify a
different inventory file at the command line using the ~-i <path>~ option or in
a configuration file using the ~inventory~ key.

As your inventory expands, you may need more than a single file to organize your
hosts and groups. Some alternatives are:

- You can create a directory with multiple inventory files
- You can pull inventory dynamically. For example, you can use a dynamic
  inventory plugin to list resources in one or more cloud providers
- You can use multiple sources for inventory, including both dynamic inventory
  and static files



* Playbooks

Playbooks are automation blueprints, in ~.yaml~ format, that Ansible uses to
deploy and configure managed nodes.

- Playbook :: A list of plays that define the order in which Ansible performs
  operations, from top to bottom, to achieve an overall goal.
- Play :: An ordered list of tasks that maps to managed nodes in an inventory.
- Task :: A reference to a single module that defines the operations that
  Ansible performs.
- Module :: A unit of code or binary that Ansible runs on managed nodes. Ansible
  modules are grouped in collections with a Fully Qualified Collection Name
  (FQCN) for each module.

Try running the following playbook:

[[file:examples/001_hello_world.yaml][001_hello_world.yaml]]

#+BEGIN_SRC yaml
- name: My first play
  hosts: ubuntus # Run on all machines in the ubuntus group
  tasks:
   - name: Ping my hosts
     ansible.builtin.ping:

   - name: Print message
     ansible.builtin.debug:
      msg: Hello world
#+END_SRC

Run it with:

#+BEGIN_SRC bash :noeval
ansible-playbook examples/001_hello_world.yaml
#+END_SRC

In the output you will see your tasks being run as well as an ~Gathering Facts~
task that is run implicitly. By default, Ansible gathers information about your
inventory that it can use in the playbook.

Th play recap summarizes the results of all tasks in the playbook per host. In
this example, there are three tasks so ~ok=3~ indicates that each task ran
successfully.

* Old
** Ad-hoc commands

#+BEGIN_SRC bash :noeval
ansible -i inventory example -m ping -u centos
ansible -i inventory example -m ping -u ansibleuser --key-file ../../dockerenv/id_rsa

# If we add the key in the inventory file we can omit the key
ansible -i inventory example -m ping -u ansibleuser

# We can even add the user to the inventory file
ansible -i inventory example -m ping

# With an ansible.cfg file we can point to our inventory file and then
# we can omit the -i option as well
ansible ubuntu-server -m ping

# -m is for module
ansible ubuntu-server -m ping

# default for -m is "command" and -a feeds the module arguments
ansible ubuntu-server -a "ls -la"
ansible ubuntu-server -a "date"

ansible multi -a "hostname"

# Control parallellism with -f (default set to 5)
ansible multi -a "hostname" -f 1

# Return everything that ansible can find about a server. Something called "gather facts"
ansible multi -m setup

# Become a different user with -b/--become (default "sudo")
ansible multi -b -a "whoami"

# Install a package
ansible multi -b -m yum -a "name=ntp state=present"

# Check that the service is runnnig / enable the service
ansible multi -b -m service -a "name=ntpd state=started enabled=yes"

# The --limit command can focus on a single server instead of the whole group
#TODO

# Background tasks -B -P
ansible multi -b -B 3600 -P 0 -a "yum -y update"
# Look at ansible_job_id and results_file field
ansible multi -b -m async_status -a <ansible_job_id>

# This won't work as the command module doesn't handle pipes and redirections etc.
ansible multi -b -a "tail /var/log/messages | grep ansible-command | wc -l"

# Use shell module instead (but should be avoided)
ansible multi -b -m shell -a "tail /var/log/messages | grep ansible-command | wc -l"
#+END_SRC

Ansible is idempotent. If we run it more than one time it will still yield the
same result. The ~command~ module will always run anyway and report a ~CHANGED~
status as ansible don't know what has been done. When using other ansible
modules, ansible can know if something was updated or not.

#+BEGIN_SRC yaml
---
- name: Set up NTP on all servers.
  hosts: all
  become: yes # Run as sudo
  tasks:
    - name: Ensure NTP is installed.
      yum: name=ntp state=present
    - name: Ensure NTP is running.
      services: name= ntpd state=started enabled=yes
#+END_SRC

** Module documentation

#+BEGIN_SRC bash :noeval
ansible-doc <module_name>
#e.g.
ansible-doc service
#+END_SRC

Modules to investigate:

- cron
- git

** Playbooks

Convention to call the main playbook ~main.yml~

*** Example 1

#+BEGIN_SRC yaml
---
- name: Install Apache.
  hosts: all

  tasks:
    - name: Install Apache.
      command: yum install --quiet -y httpd httpd-devel
    - name: Copy configuration files.
      command: >
        cp src_file /path/to/target
      command: >
        cp src_file2 /path/to/target2
    - name: Start Apache and configure it to run at boot.
      command: service httpd start
    - command: chkconfig httpd on
#+END_SRC

#+BEGIN_SRC yaml
---
- name: Install Apache.
  hosts: all
  become: true # Can also be put in each task if we don't need to be root during
               # all steps. You can also provide the -b option to the
               # ansible-playbook command

  tasks:
    - name: Install Apache.
      yum:
        name:
          - httpd
          - httpd-devel
        state: present

    - name: Copy configuration files.
      copy:
        src: "{{ item.src }}" # jinja templates
        #src: "{{ item['src'] }}" # Also acceptable
        dst: "{{ item.dest }}"
        owner: root
        group: root
        mode: 0644
      with_items:
        - src: httpd.conf
          dest: /etc/httpd/conf/httpd.conf
        - src: httpd-vhosts.conf
          dest: /etc/httpd/conf/httpd-vhosts.conf

    - name: Make sure Apache is started now and at boot.
      service:
        name: httpd
        state: started
        enabled: true
#+END_SRC

This playbook is idempotent but if any of the copied file is changed later on
the web server won't restart automatically!

#+BEGIN_SRC bash :noeval
ansible-playbook -i inventory main.yml

ansbile-playbook -i inventory multi --limit=192.168.60.5
ansbile-playbook -i inventory multi --limit=!:db

ansible-inventory --list -i inventory
#+END_SRC

*** Example 2

#+BEGIN_SRC yaml
---
- hosts: solr
  become: true

  vars_files:
    - vars.yaml

  pre_tasks:
    - name: Update apt cache if needed
      apt: update_cache=true cache_valid_time=3600

  handler:
    # A task can trigger this if it has been updated by using "notify: restart solr"
    # It's not used in the example below though
    - name: restart solr
      services: name=solr state=restarted

  tasks:
    - name: Install Java
      apt: name=openjdk-8.jdk state=present

    - name: Download solr.
      get_url:
        url: "http://fake.url/path/{{ solr_version }}/download/solr-{{ solr_version }}.tgz"
        dest: "{{ download_dir }}/solr-{{ solr_version }}.tgz" # It's a good idea to state the whole path
                                                               # so ansible can check it it already exists
        checksum: "{{ solr_checksum }}"

    - name: Expand solr.
      unarchive:
        src: "{{ download_dir }}/solr-{{ solr_version }}.tgz"
        dest: "{{ download_dir }}"
        remote_src: true # Be default it takes the file on my local machine and copies it to the remove.
                         # This tells ansible that the file is on the remote already
        # Controls idempotece by specifying which files will be created by this action
        creates: "{{ download_dir }}/solr-{{ solr_version }}/README.txt"

    - name: Run Solr insallation script.
      command: >
        {{ download_dir }}/solr-{{ solr_version }}/bin/install_solr.sh
        {{ download_dir }}/solr-{{ solr_version }}.tgz
        -i /opt
        -d /var/solr
        -u solr
        -s solr
        -p 8983
        creates={{ solr_dir }}/bin/solr

    - name: Ensure solr is started and enabled at boot.
      service: name=solr state=started enabled=yes
#+END_SRC

#+BEGIN_SRC yaml
---
download_dir: /tmp
solr_dir: /opt/solr
solr_version: 8.5.0
solr_checksum: sha512:abc123
#+END_SRC

Check if it's valid:

#+BEGIN_SRC bash :noeval
ansible-playbook -i inventory main.yml --syntax-check
#+END_SRC

*** Example 3

#+BEGIN_SRC yaml
---
- name: Install Apache.
  hosts: all
  become: true

  vars:
    proxy_vars:
      http_proxy: http://example-proxy:80/
      https_proxy: https://example-proxy:80/

  environment:
    # Set's environment for all tasks
    var0: value0
    var1: value1

  handler:
    # A handler works like a normal task and can also use notify to trigger other handlers
    - name: restart apache
      service:
        name: httpd
        state: restarted
      #notify: restart memcached

  tasks:
    - name: Download a file.
      get_url:
        url: http://ipv4.download.thinkbroadband.com/20MB.zip
        dest: /tmp
      environment:
        http_proxy: http://example-proxy:80/
        https_proxy: https://example-proxy:80/
      # or
      #environment: proxy_vars


    - name: Add an environment variable to the remote user's shell.
      lineinefile:
        dest: "~/.bash_profile"
        regexp: '^ENV_VAR='
        line: 'ENV_VAR=value'
      become: false

    - name: Get the value of an environment variable.
      shell: 'source ~/.bash_profile && echo $ENV_VAR'
      register: foo

    - debug: msg="The variable is {{ foo.stdout }}"

    - name: Install Apache.
      yum:
        name: httpd
        state: present

    - name: Copy test config file.
      copy:
        src: files/test.conf
        dst: /etc/httpd/conf.d/test.conf
      # Run the "restart apache" handler if this task has been run. The handler will be run
      # after all tasks are done
      notify:
        # List of handlers
        - restart apache

    # With this meta task we will run all handler to be run directly instead of in the end
    - name: Make sure handlers are flushed immediately.
      meta: flush_handlers

    - name: Make sure Apache is started now and at boot.
      service:
        name: httpd
        state: started
        enabled: true
#+END_SRC

#+BEGIN_SRC xml
<LocationMatch "^/+$">
  Options -Indexes
  ErrorDocument 403 /.noindex.html
</LocationMatch>

<Directory /var/www/html>
  AllowOverride None
  Require all granted
</Directory>
#+END_SRC

If a task fails before a handler has been run it will not execute. So if you
notify in one step but a later task fails, the handler will not be run in the
end of the playbook. Try it out with the ~fail~ module:

#+BEGIN_SRC yaml
tasks:
  ...
  - fail:
  ...
#+END_SRC

You can overcome this behaviour by running ~ansible-playbook~ with
~--force-handlers~.

*** Example 4

#+BEGIN_SRC yaml
---
- name: Install Apache.
  hosts: all
  #gather_facts: false # Will not make ansible_os_family available
  become: true

  #vars:
  #  apache_package: httpd
  #  apache_service: httpd
  #  apache_config_dir: /etc/apache2/sites-enabled

  handler:
    # A handler works like a normal task and can also use notify to trigger other handlers
    - name: restart apache
      service:
        name: "{{ apache_service }}"
        state: restarted
      #notify: restart memcached

  pre_tasks:
    - debug: var=ansible_os_family

    - name: Load variables files.
      include_vars: "{{ item }}"
      with_first_found:
        - "vars/apache_{{ ansible_os_family }}.yml"
        - "vars/apache_default.yml"

  tasks:
    - name: Install Apache.
      package:
        name: "{{ apache_package }}"
        state: present
      register: foo

    - debug: var=foo
    - debug: var=foo.rc
    - debug: var=foo['rc']

    - name: Copy test config file.
      copy:
        src: files/test.conf
        dst: "{{ apache_config_dir }}/test.conf"
      # Run the "restart apache" handler if this task has been run. The handler will be run
      # after all tasks are done
      notify:
        # List of handlers
        - restart apache

    # With this meta task we will run all handler to be run directly instead of in the end
    - name: Make sure handlers are flushed immediately.
      meta: flush_handlers

    - name: Make sure Apache is started now and at boot.
      service:
        name: "{{ apache_service }}"
        state: started
        enabled: true
#+END_SRC

#+BEGIN_SRC yaml
# vars/apache_default.yml
apache_package: apache2
apache_service: apache2
apache_config_dir: /etc/apache2/sites-enabled
#+END_SRC

#+BEGIN_SRC yaml
# vars/apache_RedHat.yml
apache_package: httpd
apache_service: httpd
apache_config_dir: /etc/httpd/conf.d
#+END_SRC

The ~ansible_os_family~ is set during the ~gather_facts~ step. You can see
everything ansible knows about the system by using the ~setup~ module:

#+BEGIN_SRC bash :noeval
ansible -i inventory centos -m setup
#+END_SRC

*** Other keywords to investigate

- ~when~: Control if the task should be run
- ~changed_when~: Interpret yourself if the task resulted in a change
- ~failed_when~: Interpret yourself if the task resulted in a fail
- ~ignore_error~:
- ~tags~: Tag a number of task and control which tasks should be run with ~--tags~
- blocks: Allows you to do try except workflows

** Ansible vault

#+BEGIN_SRC yaml
---
- hosts: localhost
  connection: local
  gather_facts: no

  vars_files:
    - vars/api_key.yml

  tasks:
    - name: Echo the API key which was injected into the env.
      shell: echo $API_KEY
      environment:
        API_KEY: "{{ myapp_api_key }}"
      register: echo_result

    - names: Show the result.
      debug: var=echo_result.stdout
#+END_SRC

Encrypt a var file

#+BEGIN_SRC bash :noeval
ansible-vault encrypt vars/api_key.yml
# Provide password
#+END_SRC

Use it:

#+BEGIN_SRC bash :noeval
ansible-playbook main.yml --ask-vault-pass
ansible-playbook main.yml --vault-password-file path/to/file
#+END_SRC

Decrypt file

#+BEGIN_SRC bash :noeval
ansible-vault decrypt vars/api_key.yml
#+END_SRC

Edit file without decrypting it to separate file

#+BEGIN_SRC bash :noeval
ansible-vault edit vars/api_key.yml
#+END_SRC

Change password key:

#+BEGIN_SRC bash :noeval
ansible-vault rekey vars/api_key.yml
#+END_SRC

** Playbook organization

Tasks can be included in a playbook.

#+BEGIN_SRC yaml
---
- name: Install Apache.
  hosts: all
  become: true

  handler:
    # Basically this import will replace this line with the content of apache.yml
    # so I guess that ordering is still important of imports
    - import_tasks: handlers/apache.yml

  pre_tasks:
    - name: Load variables files.
      include_vars: "{{ item }}"
      with_first_found:
        - "vars/apache_{{ ansible_os_family }}.yml"
        - "vars/apache_default.yml"

  tasks:
    - import_tasks: tasks/apache.yml
      #vars:
      #  apache_package: apache3
    # There's also something called include_tasks
    #- include_tasks: tasks/log.yml

#- import_playbook: app.yml
#+END_SRC

#+BEGIN_SRC yaml
# handlers/apache.yml
---
- name: restart apache
  service:
    name: "{{ apache_service }}"
    state: restarted
#+END_SRC

#+BEGIN_SRC yaml
# tasks/apache.yml
---
- name: Install Apache.
  package:
    name: "{{ apache_package }}"
    state: present

- name: Copy test config file.
  copy:
    src: files/test.conf
    dst: "{{ apache_config_dir }}/test.conf"
  notify:
    - restart apache

- name: Make sure Apache is started now and at boot.
  service:
    name: "{{ apache_service }}"
    state: started
    enabled: true
#+END_SRC

You can also import a playbook using ~import_playbook~

** Roles

Roles let's you package up stuff which can be used for a single or multiple
playbooks.

** Config
*** ~ansible.cfg~

#+BEGIN_SRC
[ssh_connection]
pipelining = True
#+END_SRC

*** Inventory format

#+BEGIN_SRC ini
# Application servers
[app]
192.168.60.4
192.168.60.5

# Database servers
[db]
192.168.60.6

# Group has all the servers
[multi:children]
app
db

[multi:vars]
ansible_ssh_user=ansibleuser
ansible_host=localhost
#ansible_ssh_common_args="-o StrictHostKeyChecking=no"
#+END_SRC
